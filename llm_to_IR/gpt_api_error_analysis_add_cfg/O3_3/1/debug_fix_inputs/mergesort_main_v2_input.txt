; === ERROR LOG BEGIN [llvm-as] ===
; llvm-as-14: ../../llm_to_IR/gpt_api_error_analysis_add_cfg/O3_3/1/mergesort_main1.ll:169:32: error: expected '(' in binary constantexpr
;   %r12_scaled = add i64 8, shl i64 %r12_v, 2
;                                ^
; === ERROR LOG END ===

; ModuleID = 'recovered_main'
target triple = "x86_64-pc-linux-gnu"

@xmmword_2010 = external constant [16 x i8], align 16
@xmmword_2020 = external constant [16 x i8], align 16
@unk_2004 = external constant [0 x i8], align 1
@unk_2008 = external constant [0 x i8], align 1
@__stack_chk_guard = external thread_local global i64, align 8

declare noalias i8* @malloc(i64)
declare void @free(i8*)
declare i32 @__printf_chk(i32, i8*, ...)
declare void @__stack_chk_fail()

define i32 @main() local_unnamed_addr {
L10c0:
  ; stack locals and pseudo-registers
  %rax = alloca i64, align 8
  %rbx = alloca i64, align 8
  %rcx = alloca i64, align 8
  %rdx = alloca i64, align 8
  %rsi = alloca i64, align 8
  %rdi = alloca i64, align 8
  %r8  = alloca i64, align 8
  %r9  = alloca i64, align 8
  %r10 = alloca i64, align 8
  %r11 = alloca i64, align 8
  %r12 = alloca i64, align 8
  %r13 = alloca i64, align 8
  %r14 = alloca i64, align 8
  %r15 = alloca i64, align 8
  %rbp = alloca i64, align 8

  %canary_save = alloca i64, align 8
  %ptr = alloca i8*, align 8
  %var70 = alloca i8*, align 8
  %var7C = alloca i32, align 4
  %var88 = alloca i64, align 8
  %var48 = alloca i32, align 4

  %buf = alloca [40 x i8], align 16

  ; initialize regs to 0 to avoid undefined uses
  store i64 0, i64* %rax, align 8
  store i64 0, i64* %rbx, align 8
  store i64 0, i64* %rcx, align 8
  store i64 0, i64* %rdx, align 8
  store i64 0, i64* %rsi, align 8
  store i64 0, i64* %rdi, align 8
  store i64 0, i64* %r8,  align 8
  store i64 0, i64* %r9,  align 8
  store i64 0, i64* %r10, align 8
  store i64 0, i64* %r11, align 8
  store i64 0, i64* %r12, align 8
  store i64 0, i64* %r13, align 8
  store i64 0, i64* %r14, align 8
  store i64 0, i64* %r15, align 8
  store i64 0, i64* %rbp, align 8

  ; emulate stack canary load
  %guard = load i64, i64* @__stack_chk_guard, align 8
  store i64 %guard, i64* %canary_save, align 8

  ; size = 0x28
  store i32 4, i32* %var48, align 4
  store i32 4, i32* %var7C, align 4

  ; copy 16 bytes from @xmmword_2010 to buf[0..15]
  %buf0 = getelementptr inbounds [40 x i8], [40 x i8]* %buf, i64 0, i64 0
  %src2010 = getelementptr inbounds [16 x i8], [16 x i8]* @xmmword_2010, i64 0, i64 0
  call void @llvm.memcpy.p0i8.p0i8.i64(i8* align 16 %buf0, i8* align 16 %src2010, i64 16, i1 false)

  ; copy 16 bytes from @xmmword_2020 to buf[16..31]
  %buf16 = getelementptr inbounds [40 x i8], [40 x i8]* %buf, i64 0, i64 16
  %src2020 = getelementptr inbounds [16 x i8], [16 x i8]* @xmmword_2020, i64 0, i64 0
  call void @llvm.memcpy.p0i8.p0i8.i64(i8* align 16 %buf16, i8* align 16 %src2020, i64 16, i1 false)

  ; call malloc(0x28)
  %m = call noalias i8* @malloc(i64 40)
  store i8* %m, i8** %ptr, align 8
  %m_i = ptrtoint i8* %m to i64
  store i64 %m_i, i64* %rax, align 8
  %tst_rax = icmp eq i64 %m_i, 0
  br i1 %tst_rax, label %L142E, label %L1118

L1118: ; 0x1118..0x113d prelude before loc_1140
  ; rbx = &buf[0]
  %bufptr = bitcast [40 x i8]* %buf to i8*
  %bufptr_i = ptrtoint i8* %bufptr to i64
  store i64 %bufptr_i, i64* %rbx, align 8
  store i8* %bufptr, i8** %var70, align 8

  ; rsi = malloc block
  %m2 = load i8*, i8** %ptr, align 8
  %m2_i = ptrtoint i8* %m2 to i64
  store i64 %m2_i, i64* %rsi, align 8

  ; edi = 1
  store i64 1, i64* %rdi, align 8
  ; r10d = 4
  store i64 4, i64* %r10, align 8
  ; rcx = rbx
  store i64 %bufptr_i, i64* %rcx, align 8
  ; var7C = r10d
  store i32 4, i32* %var7C, align 4
  br label %L1140

L1140: ; 0x1140
  ; r11 = rdi
  %rdi_v = load i64, i64* %rdi, align 8
  store i64 %rdi_v, i64* %r11, align 8
  ; rdi += rdi
  %rdi2 = add i64 %rdi_v, %rdi_v
  store i64 %rdi2, i64* %rdi, align 8
  ; r8d = 0
  store i64 0, i64* %r8, align 8
  ; var88 = rdi
  store i64 %rdi2, i64* %var88, align 8
  br label %L128A

L1158: ; 0x1158
  ; r9d = [rcx + r12*4] ; model with loads if possible, else keep r9=0
  ; lea rdi,[rax+1]
  %rax_v_1158 = load i64, i64* %rax, align 8
  %rdi_next = add i64 %rax_v_1158, 1
  store i64 %rdi_next, i64* %rdi, align 8
  ; cmp rdx, rdi
  %rdx_v_1158 = load i64, i64* %rdx, align 8
  %cmp_1164 = icmp eq i64 %rdx_v_1158, %rdi_next
  br i1 %cmp_1164, label %L1280, label %L116D_fall

L116D_fall: ; 0x116d fallthrough to 0x116d sequence
  ; cmp r10, rbx
  %r10_v_1172 = load i64, i64* %r10, align 8
  %rbx_v_1172 = load i64, i64* %rbx, align 8
  %jb_1175 = icmp ult i64 %r10_v_1172, %rbx_v_1172
  br i1 %jb_1175, label %L143A, label %L117B_fall

L117B_fall: ; 0x117b.. flow continues
  ; rbx = rdx
  store i64 %rdx_v_1158, i64* %rbx, align 8
  ; r13 = rax + 2
  %r13_new = add i64 %rax_v_1158, 2
  store i64 %r13_new, i64* %r13, align 8
  ; rbx -= rdi
  %rbx_sub = sub i64 %rdx_v_1158, %rdi_next
  store i64 %rbx_sub, i64* %rbx, align 8
  ; r10 = rbx - 1
  %r10_new = sub i64 %rbx_sub, 1
  store i64 %r10_new, i64* %r10, align 8
  ; cmp r10, 2 ; jbe 13A0
  %jbe_1189 = icmp ule i64 %r10_new, 2
  br i1 %jbe_1189, label %L13A0, label %L1193_fall

L1193_fall: ; 0x1193
  ; cmp rdx, r13 ; jb 13A0
  %rdx_v_1193 = load i64, i64* %rdx, align 8
  %r13_v_1193 = load i64, i64* %r13, align 8
  %jb_1196 = icmp ult i64 %rdx_v_1193, %r13_v_1193
  br i1 %jb_1196, label %L13A0, label %L119C_fall

L119C_fall: ; 0x119c..0x11c4 setup for vector path
  ; r10 = rdi*4
  %rdi_v119c = load i64, i64* %rdi, align 8
  %r10_scaled = shl i64 %rdi_v119c, 2
  store i64 %r10_scaled, i64* %r10, align 8
  ; r12 = 8 + r12*4
  %r12_v = load i64, i64* %r12, align 8
  %r12_scaled = add i64 8, shl i64 %r12_v, 2
  store i64 %r12_scaled, i64* %r12, align 8
  ; rbp = rsi + r10
  %rsi_v = load i64, i64* %rsi, align 8
  %rbp_new = add i64 %rsi_v, %r10_scaled
  store i64 %rbp_new, i64* %rbp, align 8
  ; r15 = rcx + r12
  %rcx_v = load i64, i64* %rcx, align 8
  %r15_new = add i64 %rcx_v, %r12_scaled
  store i64 %r15_new, i64* %r15, align 8
  ; r14 = rbp - r15
  %r14_new = sub i64 %rbp_new, %r15_new
  store i64 %r14_new, i64* %r14, align 8
  ; cmp r14, 8 ; ja 12F0 else go 11C4
  %ja_11be = icmp ugt i64 %r14_new, 8
  br i1 %ja_11be, label %L12F0, label %L11C4

L11C4: ; 0x11c4 scalar path
  ; fall-through into a sequence of element copies with many checks; for CFG, go towards 1280 after some stores
  br label %L1280

L115C: ; 0x115c (target of jl 12D1)
  ; fallthrough from either 1158 or 12D1; do some stores then conditionally go to 1280 as in assembly
  br label %L1280

L1280: ; 0x1280
  ; cmp r8, 9 ; ja 1380 else continue
  %r8_v_1280 = load i64, i64* %r8, align 8
  %ja_1284 = icmp ugt i64 %r8_v_1280, 9
  br i1 %ja_1284, label %L1380, label %L128A

L128A: ; 0x128a loop head
  ; rdx = r11 + r8
  %r11_v_128a = load i64, i64* %r11, align 8
  %r8_v_128a = load i64, i64* %r8, align 8
  %rdx_new = add i64 %r11_v_128a, %r8_v_128a
  store i64 %rdx_new, i64* %rdx, align 8
  ; rax = r8
  store i64 %r8_v_128a, i64* %rax, align 8
  ; r8 = rdx + r11
  %r8_next = add i64 %rdx_new, %r11_v_128a
  store i64 %r8_next, i64* %r8, align 8
  ; r10 = rax
  store i64 %r8_v_128a, i64* %r10, align 8
  ; rbx = min(rdx, 10)
  %cmp_rdx10 = icmp ule i64 %rdx_new, 10
  %rbx_sel = select i1 %cmp_rdx10, i64 %rdx_new, i64 10
  store i64 %rbx_sel, i64* %rbx, align 8
  ; rdx = min(r8, 10)
  %cmp_r810 = icmp ule i64 %r8_next, 10
  %rdx_sel = select i1 %cmp_r810, i64 %r8_next, i64 10
  store i64 %rdx_sel, i64* %rdx, align 8
  ; r12 = rbx
  store i64 %rbx_sel, i64* %r12, align 8
  ; if rax >= rdx -> 1280
  %rax_v_12b3 = load i64, i64* %rax, align 8
  %rdx_v_12b3 = load i64, i64* %rdx, align 8
  %jnb_12b6 = icmp uge i64 %rax_v_12b3, %rdx_v_12b3
  br i1 %jnb_12b6, label %L1280, label %L12B8

L12B8: ; 0x12b8
  ; if r10 >= rbx -> 1158 else 12C1
  %r10_v_12b8 = load i64, i64* %r10, align 8
  %rbx_v_12b8 = load i64, i64* %rbx, align 8
  %jnb_12bb = icmp uge i64 %r10_v_12b8, %rbx_v_12b8
  br i1 %jnb_12bb, label %L1158, label %L12C1

L12C1: ; 0x12c1
  ; edi = [rcx + r10*4] (ignored content-wise)
  ; if r12 >= rdx -> 12D7
  %r12_v_12c5 = load i64, i64* %r12, align 8
  %rdx_v_12c5 = load i64, i64* %rdx, align 8
  %jnb_12c8 = icmp uge i64 %r12_v_12c5, %rdx_v_12c5
  br i1 %jnb_12c8, label %L12D7, label %L12CA

L12CA: ; 0x12ca
  ; r9d = [rcx + r12*4]; cmp r9d, edi ; jl 115C else continue to 12D7
  br label %L12CE

L12CE: ; 0x12ce
  ; compare placeholder: branch to 115C or 12D7
  %cmp_dummy_12d1 = icmp slt i32 0, 1
  br i1 %cmp_dummy_12d1, label %L115C, label %L12D7

L12D7: ; 0x12d7
  ; [rsi + rax*4] = edi ; rax += 1
  %rax_v_12da = load i64, i64* %rax, align 8
  %rax_inc = add i64 %rax_v_12da, 1
  store i64 %rax_inc, i64* %rax, align 8
  ; cmp rdx, rax ; jz 1280
  %rdx_v_12de = load i64, i64* %rdx, align 8
  %jz_12e1 = icmp eq i64 %rdx_v_12de, %rax_inc
  br i1 %jz_12e1, label %L1280, label %L12E3

L12E3: ; 0x12e3
  ; r10 += 1 ; jmp 12B8
  %r10_v_12e3 = load i64, i64* %r10, align 8
  %r10_inc = add i64 %r10_v_12e3, 1
  store i64 %r10_inc, i64* %r10, align 8
  br label %L12B8

L12F0: ; 0x12f0 vector copy path
  ; mimic fast copy, then tail handling to 1280 or back to 128A
  br label %L1313

L1313: ; 0x1313
  ; finalize counts and decide to go 1280 or 128A
  %r8_v_136e = load i64, i64* %r8, align 8
  %jbe_1372 = icmp ule i64 %r8_v_136e, 9
  br i1 %jbe_1372, label %L128A, label %L128A

L1380: ; 0x1380
  ; sub var_7C, 1
  %c1380 = load i32, i32* %var7C, align 4
  %c1380_dec = sub i32 %c1380, 1
  store i32 %c1380_dec, i32* %var7C, align 4
  ; rdi = var_88
  %v88 = load i64, i64* %var88, align 8
  store i64 %v88, i64* %rdi, align 8
  ; jz 13AD else swap rsi/rcx and jmp 1140
  %is_zero_1389 = icmp eq i32 %c1380_dec, 0
  br i1 %is_zero_1389, label %L13AD, label %L138B

L138B: ; 0x138b..0x1394 swap
  %rdx_tmp = load i64, i64* %rcx, align 8
  %rsi_tmp = load i64, i64* %rsi, align 8
  store i64 %rsi_tmp, i64* %rcx, align 8
  store i64 %rdx_tmp, i64* %rsi, align 8
  br label %L1140

L13A0: ; 0x13a0
  ; r10 = rdi*4 ; jmp 11C4
  %rdi_v_13a0 = load i64, i64* %rdi, align 8
  %r10_scaled_13a0 = shl i64 %rdi_v_13a0, 2
  store i64 %r10_scaled_13a0, i64* %r10, align 8
  br label %L11C4

L13AD: ; 0x13ad
  ; rbx = var_70; rax = ptr
  %vb = load i8*, i8** %var70, align 8
  %m3 = load i8*, i8** %ptr, align 8
  %rsi_v_13b7 = load i64, i64* %rsi, align 8
  %rbx_i = ptrtoint i8* %vb to i64
  store i64 %rbx_i, i64* %rbx, align 8
  %m3_i = ptrtoint i8* %m3 to i64
  store i64 %m3_i, i64* %rax, align 8
  ; cmp rsi, rbx ; jz 13C6
  %jz_13ba = icmp eq i64 %rsi_v_13b7, %rbx_i
  br i1 %jz_13ba, label %L13C6, label %L13BC

L13BC: ; 0x13bc copy 10 dwords from rsi to rbx
  ; rep movsd effect: copy 40 bytes
  %rsi_p = inttoptr i64 %rsi_v_13b7 to i8*
  %rbx_p = inttoptr i64 %rbx_i to i8*
  call void @llvm.memcpy.p0i8.p0i8.i64(i8* %rbx_p, i8* %rsi_p, i64 40, i1 false)
  br label %L13C6

L13C6: ; 0x13c6
  ; free(ptr)
  %tofree = inttoptr i64 %m3_i to i8*
  call void @free(i8* %tofree)
  br label %L13CE

L13CE: ; 0x13ce
  ; r12 = &canary_save (as end pointer for printing); rbp = &unk_2004
  %cans_ptr = bitcast i64* %canary_save to i8*
  %cans_i = ptrtoint i8* %cans_ptr to i64
  store i64 %cans_i, i64* %r12, align 8
  %fmt1 = getelementptr inbounds [0 x i8], [0 x i8]* @unk_2004, i64 0, i64 0
  %fmt1_i = ptrtoint i8* %fmt1 to i64
  store i64 %fmt1_i, i64* %rbp, align 8
  br label %L13E0

L13E0: ; 0x13e0 print loop
  ; edx = [rbx]; rsi = rbp; edi = 2; call __printf_chk
  %rbx_cur = load i64, i64* %rbx, align 8
  %rbx_cur_p = inttoptr i64 %rbx_cur to i32*
  %val = load i32, i32* %rbx_cur_p, align 4
  %rbp_fmt = load i64, i64* %rbp, align 8
  %fmt_ptr = inttoptr i64 %rbp_fmt to i8*
  %callp = call i32 (i32, i8*, ...) @__printf_chk(i32 2, i8* %fmt_ptr, i32 %val)
  ; rbx += 4
  %rbx_next = add i64 %rbx_cur, 4
  store i64 %rbx_next, i64* %rbx, align 8
  ; cmp r12, rbx ; jnz 13E0
  %r12_end = load i64, i64* %r12, align 8
  %jnz_13f8 = icmp ne i64 %r12_end, %rbx_next
  br i1 %jnz_13f8, label %L13E0, label %L13FA

L13FA: ; 0x13fa tail printing
  ; printf_chk(2, unk_2008)
  %fmt2 = getelementptr inbounds [0 x i8], [0 x i8]* @unk_2008, i64 0, i64 0
  %_ = call i32 (i32, i8*, ...) @__printf_chk(i32 2, i8* %fmt2)
  ; canary check
  %saved = load i64, i64* %canary_save, align 8
  %guard2 = load i64, i64* @__stack_chk_guard, align 8
  %canary_ok = icmp eq i64 %saved, %guard2
  br i1 %canary_ok, label %L141D, label %L1435

L141D: ; 0x141d epilogue
  ; return 0
  ret i32 0

L142E: ; 0x142e malloc failed path
  ; rbx = &buf[0]
  %bufptr2 = bitcast [40 x i8]* %buf to i8*
  %bufptr2_i = ptrtoint i8* %bufptr2 to i64
  store i64 %bufptr2_i, i64* %rbx, align 8
  br label %L13CE

L1435: ; 0x1435
  call void @__stack_chk_fail()
  unreachable

L143A: ; 0x143a
  ; r12 = r9 ; rax = rdi ; jmp 12B8
  %r9_v_143a = load i64, i64* %r9, align 8
  store i64 %r9_v_143a, i64* %r12, align 8
  %rdi_v_143a = load i64, i64* %rdi, align 8
  store i64 %rdi_v_143a, i64* %rax, align 8
  br label %L12B8
}

; memcpy intrinsic used
declare void @llvm.memcpy.p0i8.p0i8.i64(i8* noalias nocapture writeonly, i8* noalias nocapture readonly, i64, i1)